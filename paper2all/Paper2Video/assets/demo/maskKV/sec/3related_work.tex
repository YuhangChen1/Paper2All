\section{Related Work}
\subsection{Diffusion Models for Language}
Diffusion large language models (dLLMs) have emerged as a compelling non-autoregressive paradigm for text generation~\citep{li2025survey}. Their core mechanism involves a progressive refinement process, where a sequence is generated by iteratively denoising a noise-corrupted input over a series of discrete steps.
Recent works have demonstrated both the scalability of this architecture ~\citep{nie2025large} and the effectiveness of training techniques such as AR-based initialization and context-adaptive noise scheduling~\citep{ye2025dream}, achieving competitive performance.

The development of diffusion LLMs is now driven by the twin objectives of accelerating inference and improving generation quality.
To accelerate inference, Fast-dLLM~\citep{wu2025fast} introduces a block-wise approximate KV cache tailored for dLLM, while dLLM-Cache ~\citep{liu2025dllm} leverages feature caching to reduce redundant computation. SlowFast Sampling~\citep{wei2025accelerating} jointly considers confidence, convergence and position  for dynamic decoding, achieving a practical trade-off between speed and quality.
For enhancing performance, DEADAL~\citep{li2025beyond} addressed the limitation of fixed-length generation by introducing variable-length denoising.
LongLLaDA~\citep{liu2025longllada} extends diffusion LLMs to long contexts. These advancements have driven the demand for longer-context handling, which under caching mechanisms leads to prohibitive memory overhead.

\subsection{KV Cache Compression}
The substantial memory footprint of the Key-Value (KV) cache presents a primary bottleneck for long-context inference in Large Language Models (LLMs). 
Early approaches such as LongFormer~\citep{beltagy2020longformer} and StreamingLLM~\citep{xiao2023efficient} used static, content-agnostic rules (e.g., sliding windows, attention sinks), but their tendency to drop long-range information spurred interest in content-aware, dynamic eviction policies~\citep{zhang2023h2o, li2024snapkv, devoto2024simple}. 
Prominent examples include H2O~\citep{zhang2023h2o}, which greedily evicts KVs with low historical attention scores; SnapKV~\citep{li2024snapkv}, which filters for key KVs during the prefill phase.
Building on the recognition of specialized model components, recent studies have explored finer-grained and adaptive budget allocation strategies~\citep{cai2024pyramidkv,wang2024squeezeattention,feng2024ada,xiao2024duoattention}. 
At the layer level, methods either vary cache sizes across layers or dynamically reallocate budgets according to prompt-specific importance~\citep{cai2024pyramidkv,wang2024squeezeattention}. 
At the head level, approaches assign differentiated budgets or categorize heads into functional roles for selective caching~\citep{feng2024ada,xiao2024duoattention}. 
While effective for ARMs, these techniques depend on sequential decoding and are incompatible with the iterative inference of dLLMs. 
Existing attempts, such as Sparse-dLLM, provide only preliminary insights into sparse caching in dLLMs. 
We thus propose a more comprehensive and fine-grained framework for KV cache eviction for dLLMs.
