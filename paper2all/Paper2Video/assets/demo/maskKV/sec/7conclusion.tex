\section{Conclusion}
We explored dLLM characteristics and introduced \mymethod, a training‑free framework enabling fine‑grained cache eviction via Mask Voting and adaptive layer–head budget allocation. Experiments show that \mymethod~reduces the KV cache to 256 tokens while retaining up to 94\% of original performance, highlighting an efficient trade‑off for long‑context inference.

