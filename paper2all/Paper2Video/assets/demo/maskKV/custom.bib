@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  volume={1},
  number={2},
  year={2023}
}

@article{nie2025large,
  title={Large language diffusion models},
  author={Nie, Shen and Zhu, Fengqi and You, Zebin and Zhang, Xiaolu and Ou, Jingyang and Hu, Jun and Zhou, Jun and Lin, Yankai and Wen, Ji-Rong and Li, Chongxuan},
  journal={arXiv preprint arXiv:2502.09992},
  year={2025}
}

@article{chen2025dpad,
  title={DPad: Efficient Diffusion Language Models with Suffix Dropout},
  author={Chen, Xinhua and Huang, Sitao and Guo, Cong and Wei, Chiyue and He, Yintao and Zhang, Jianyi and Li, Hai and Chen, Yiran and others},
  journal={arXiv preprint arXiv:2508.14148},
  year={2025}
}

@article{wang2025diffusion,
  title={Diffusion llms can do faster-than-ar inference via discrete diffusion forcing},
  author={Wang, Xu and Xu, Chenkai and Jin, Yijie and Jin, Jiachun and Zhang, Hao and Deng, Zhijie},
  journal={arXiv preprint arXiv:2508.09192},
  year={2025}
}

@article{khanna2025mercury,
  title={Mercury: Ultra-fast language models based on diffusion},
  author={Khanna, Samar and Kharbanda, Siddhant and Li, Shufan and Varma, Harshit and Wang, Eric and Birnbaum, Sawyer and Luo, Ziyang and Miraoui, Yanis and Palrecha, Akash and Ermon, Stefano and others},
  journal={arXiv preprint arXiv:2506.17298},
  year={2025}
}

@article{song2025seed,
  title={Seed diffusion: A large-scale diffusion language model with high-speed inference},
  author={Song, Yuxuan and Zhang, Zheng and Luo, Cheng and Gao, Pengyang and Xia, Fan and Luo, Hao and Li, Zheng and Yang, Yuehang and Yu, Hongli and Qu, Xingwei and others},
  journal={arXiv preprint arXiv:2508.02193},
  year={2025}
}

@misc{genimid,
    title = {Gemini Diffusion, our state-of-the-art, experimental text diffusion model},
    url = {https://deepmind.google/models/gemini-diffusion/},
    author = {Gemini},
    year = {2025}
}

@article{ye2025dream,
  title={Dream 7b: Diffusion large language models},
  author={Ye, Jiacheng and Xie, Zhihui and Zheng, Lin and Gao, Jiahui and Wu, Zirui and Jiang, Xin and Li, Zhenguo and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2508.15487},
  year={2025}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{liu2025dllm,
  title={dllm-cache: Accelerating diffusion large language models with adaptive caching},
  author={Liu, Zhiyuan and Yang, Yicun and Zhang, Yaojie and Chen, Junjie and Zou, Chang and Wei, Qingyuan and Wang, Shaobo and Zhang, Linfeng},
  journal={arXiv preprint arXiv:2506.06295},
  year={2025}
}

@article{wu2025fast,
  title={Fast-dllm: Training-free acceleration of diffusion llm by enabling kv cache and parallel decoding},
  author={Wu, Chengyue and Zhang, Hao and Xue, Shuchen and Liu, Zhijian and Diao, Shizhe and Zhu, Ligeng and Luo, Ping and Han, Song and Xie, Enze},
  journal={arXiv preprint arXiv:2505.22618},
  year={2025}
}

@article{hu2025accelerating,
  title={Accelerating diffusion language model inference via efficient kv caching and guided diffusion},
  author={Hu, Zhanqiu and Meng, Jian and Akhauri, Yash and Abdelfattah, Mohamed S and Seo, Jae-sun and Zhang, Zhiru and Gupta, Udit},
  journal={arXiv preprint arXiv:2505.21467},
  year={2025}
}

@article{ma2025dkv,
  title={dkv-cache: The cache for diffusion language models},
  author={Ma, Xinyin and Yu, Runpeng and Fang, Gongfan and Wang, Xinchao},
  journal={arXiv preprint arXiv:2505.15781},
  year={2025}
}

@article{zhang2023h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={34661--34710},
  year={2023}
}

@article{li2024snapkv,
  title={Snapkv: Llm knows what you are looking for before generation},
  author={Li, Yuhong and Huang, Yingbing and Yang, Bowen and Venkitesh, Bharat and Locatelli, Acyr and Ye, Hanchen and Cai, Tianle and Lewis, Patrick and Chen, Deming},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={22947--22970},
  year={2024}
}

@article{song2025sparse,
  title={Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction},
  author={Song, Yuerong and Liu, Xiaoran and Li, Ruixiao and Liu, Zhigeng and Huang, Zengfeng and Guo, Qipeng and He, Ziwei and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2508.02558},
  year={2025}
}

@article{wang2024squeezeattention,
  title={Squeezeattention: 2d management of kv-cache in llm inference via layer-wise optimal budget},
  author={Wang, Zihao and Cui, Bin and Gan, Shaoduo},
  journal={arXiv preprint arXiv:2404.04793},
  year={2024}
}

@article{feng2024ada,
  title={Ada-kv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference},
  author={Feng, Yuan and Lv, Junlin and Cao, Yukun and Xie, Xike and Zhou, S Kevin},
  journal={arXiv preprint arXiv:2407.11550},
  year={2024}
}

@article{li2025survey,
  title={A survey on diffusion language models},
  author={Li, Tianyi and Chen, Mingda and Guo, Bowei and Shen, Zhiqiang},
  journal={arXiv preprint arXiv:2508.10875},
  year={2025}
}

@article{wei2025accelerating,
  title={Accelerating Diffusion Large Language Models with SlowFast: The Three Golden Principles},
  author={Wei, Qingyan and Zhang, Yaojie and Liu, Zhiyuan and Liu, Dongrui and Zhang, Linfeng},
  journal={arXiv preprint arXiv:2506.10848},
  year={2025}
}

@article{li2025beyond,
  title={Beyond Fixed: Training-Free Variable-Length Denoising for Diffusion Large Language Models},
  author={Li, Jinsong and Dong, Xiaoyi and Zang, Yuhang and Cao, Yuhang and Wang, Jiaqi and Lin, Dahua},
  journal={arXiv preprint arXiv:2508.00819},
  year={2025}
}

@article{liu2025longllada,
  title={LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs},
  author={Liu, Xiaoran and Liu, Zhigeng and Huang, Zengfeng and Guo, Qipeng and He, Ziwei and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2506.14429},
  year={2025}
}

@article{zhao2025d1,
  title={d1: Scaling reasoning in diffusion large language models via reinforcement learning},
  author={Zhao, Siyan and Gupta, Devaansh and Zheng, Qinqing and Grover, Aditya},
  journal={arXiv preprint arXiv:2504.12216},
  year={2025}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@article{devoto2024simple,
  title={A Simple and Effective $ L\_2 $ Norm-Based Strategy for KV Cache Compression},
  author={Devoto, Alessio and Zhao, Yu and Scardapane, Simone and Minervini, Pasquale},
  journal={arXiv preprint arXiv:2406.11430},
  year={2024}
}

@article{cai2024pyramidkv,
  title={Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling},
  author={Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Li, Yucheng and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Hu, Junjie and others},
  journal={arXiv preprint arXiv:2406.02069},
  year={2024}
}

@article{xiao2024duoattention,
  title={Duoattention: Efficient long-context llm inference with retrieval and streaming heads},
  author={Xiao, Guangxuan and Tang, Jiaming and Zuo, Jingwei and Guo, Junxian and Yang, Shang and Tang, Haotian and Fu, Yao and Han, Song},
  journal={arXiv preprint arXiv:2410.10819},
  year={2024}
}

@article{xu2025dllmquant,
  title={DLLMQuant: Quantizing Diffusion-based Large Language Models},
  author={Xu, Chen and Yang, Dawei},
  journal={arXiv preprint arXiv:2508.14090},
  year={2025}
}

@article{he2024matters,
  title={What matters in transformers? not all attention is needed},
  author={He, Shwai and Sun, Guoheng and Shen, Zheyu and Li, Ang},
  journal={arXiv preprint arXiv:2406.15786},
  year={2024}
}

@article{bai2023longbench,
  title={Longbench: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}

@article{kovcisky2018narrativeqa,
  title={The narrativeqa reading comprehension challenge},
  author={Ko{\v{c}}isk{\`y}, Tom{\'a}{\v{s}} and Schwarz, Jonathan and Blunsom, Phil and Dyer, Chris and Hermann, Karl Moritz and Melis, G{\'a}bor and Grefenstette, Edward},
  journal={Transactions of the Association for Computational Linguistics},
  volume={6},
  pages={317--328},
  year={2018},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{dasigi2021dataset,
  title={A dataset of information-seeking questions and answers anchored in research papers},
  author={Dasigi, Pradeep and Lo, Kyle and Beltagy, Iz and Cohan, Arman and Smith, Noah A and Gardner, Matt},
  journal={arXiv preprint arXiv:2105.03011},
  year={2021}
}

@article{yang2018hotpotqa,
  title={HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  journal={arXiv preprint arXiv:1809.09600},
  year={2018}
}

@article{ho2020constructing,
  title={Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps},
  author={Ho, Xanh and Nguyen, Anh-Khoa Duong and Sugawara, Saku and Aizawa, Akiko},
  journal={arXiv preprint arXiv:2011.01060},
  year={2020}
}

@article{trivedi2022musique,
  title={MuSiQue: Multihop Questions via Single-hop Question Composition},
  author={Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={539--554},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{huang2021efficient,
  title={Efficient attentions for long document summarization},
  author={Huang, Luyang and Cao, Shuyang and Parulian, Nikolaus and Ji, Heng and Wang, Lu},
  journal={arXiv preprint arXiv:2104.02112},
  year={2021}
}

@article{zhong2021qmsum,
  title={QMSum: A new benchmark for query-based multi-domain meeting summarization},
  author={Zhong, Ming and Yin, Da and Yu, Tao and Zaidi, Ahmad and Mutuma, Mutethia and Jha, Rahul and Awadallah, Ahmed Hassan and Celikyilmaz, Asli and Liu, Yang and Qiu, Xipeng and others},
  journal={arXiv preprint arXiv:2104.05938},
  year={2021}
}

@article{fabbri2019multi,
  title={Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model},
  author={Fabbri, Alexander R and Li, Irene and She, Tianwei and Li, Suyi and Radev, Dragomir R},
  journal={arXiv preprint arXiv:1906.01749},
  year={2019}
}

@inproceedings{li2002learning,
  title={Learning question classifiers},
  author={Li, Xin and Roth, Dan},
  booktitle={COLING 2002: The 19th International Conference on Computational Linguistics},
  year={2002}
}

@article{gliwa2019samsum,
  title={SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization},
  author={Gliwa, Bogdan and Mochol, Iwona and Biesek, Maciej and Wawer, Aleksander},
  journal={arXiv preprint arXiv:1911.12237},
  year={2019}
}

@article{joshi2017triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@inproceedings{guo2023longcoder,
  title={Longcoder: A long-range pre-trained language model for code completion},
  author={Guo, Daya and Xu, Canwen and Duan, Nan and Yin, Jian and McAuley, Julian},
  booktitle={International Conference on Machine Learning},
  pages={12098--12107},
  year={2023},
  organization={PMLR}
}

@article{liu2023repobench,
  title={Repobench: Benchmarking repository-level code auto-completion systems},
  author={Liu, Tianyang and Xu, Canwen and McAuley, Julian},
  journal={arXiv preprint arXiv:2306.03091},
  year={2023}
}

